import argparse
import csv
import os
from datetime import datetime

import requests

from env_utils import load_dotenv


MB_API_URL = "https://mb-api.abuse.ch/api/v1/"


def _default_data_path(name: str, fallback: str) -> str:
    if os.path.isdir("/data"):
        return os.path.join("/data", name)
    return fallback


def hunt_by_tag(api_key: str, tag: str, limit: int, verbose: bool) -> list[dict]:
    data = {"query": "get_taginfo", "tag": tag, "limit": str(limit)}
    headers = {"Auth-Key": api_key} if api_key else {}
    r = requests.post(MB_API_URL, headers=headers, data=data, timeout=60)
    r.raise_for_status()
    j = r.json()
    if j.get("query_status") != "ok":
        if verbose:
            print(f"[ERROR] MalwareBazaar query_status={j.get('query_status')!r}")
        return []
    rows = j.get("data") or []
    if not isinstance(rows, list):
        return []
    return rows


def main():
    load_dotenv()

    ap = argparse.ArgumentParser(
        description="Hunt sample hashes from MalwareBazaar (e.g., by tag) and write hash list + metadata CSV."
    )
    ap.add_argument(
        "-k",
        "--api_key",
        help="MalwareBazaar API key. If not provided, MB_API_KEY env var (and .env) will be used.",
    )
    ap.add_argument("--tag", required=True, help="MalwareBazaar tag to hunt (e.g., amadey).")
    ap.add_argument("--limit", type=int, default=100, help="Max number of results to fetch (default: 100).")
    ap.add_argument(
        "--hashes-out",
        default="",
        help="Output path for sha256 list (default: /data/<tag>_<limit>_hashes.txt in Docker).",
    )
    ap.add_argument(
        "--csv-out",
        default="",
        help="Output path for metadata csv (default: /data/<tag>_<limit>_meta.csv in Docker).",
    )
    ap.add_argument("--verbose", action="store_true")
    args = ap.parse_args()

    api_key = args.api_key or os.getenv("MB_API_KEY", "")
    if not api_key and args.verbose:
        print("[WARN] MB_API_KEY not set. Some endpoints may still work, but rate limits may be harsher.")

    tag = args.tag
    limit = max(1, int(args.limit))

    default_hashes = _default_data_path(f"{tag}_{limit}_hashes.txt", f"{tag}_{limit}_hashes.txt")
    default_csv = _default_data_path(f"{tag}_{limit}_meta.csv", f"{tag}_{limit}_meta.csv")

    hashes_out = args.hashes_out or default_hashes
    csv_out = args.csv_out or default_csv

    if args.verbose:
        print(f"[*] Hunting tag={tag} limit={limit}")

    rows = hunt_by_tag(api_key=api_key, tag=tag, limit=limit, verbose=args.verbose)
    if not rows:
        print("[WARN] No results.")
        return

    hashes: list[str] = []
    for row in rows:
        h = row.get("sha256_hash")
        if h and isinstance(h, str):
            hashes.append(h.strip())

    # de-dupe while preserving order
    seen = set()
    uniq_hashes = []
    for h in hashes:
        if h in seen:
            continue
        seen.add(h)
        uniq_hashes.append(h)

    os.makedirs(os.path.dirname(hashes_out) or ".", exist_ok=True)
    os.makedirs(os.path.dirname(csv_out) or ".", exist_ok=True)

    with open(hashes_out, "w", encoding="utf-8") as f:
        for h in uniq_hashes:
            f.write(h + "\n")

    # keep a small, stable CSV schema
    fieldnames = [
        "sha256_hash",
        "first_seen",
        "last_seen",
        "file_name",
        "file_size",
        "file_type_mime",
        "signature",
        "tags",
        "delivery_method",
        "reporter",
        "intelligence",
        "download_url",
        "generated_at",
    ]
    generated_at = datetime.utcnow().isoformat(timespec="seconds") + "Z"

    with open(csv_out, "w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        w.writeheader()
        for row in rows:
            w.writerow(
                {
                    "sha256_hash": row.get("sha256_hash", ""),
                    "first_seen": row.get("first_seen", ""),
                    "last_seen": row.get("last_seen", ""),
                    "file_name": row.get("file_name", ""),
                    "file_size": row.get("file_size", ""),
                    "file_type_mime": row.get("file_type_mime", ""),
                    "signature": row.get("signature", ""),
                    "tags": ",".join(row.get("tags") or []) if isinstance(row.get("tags"), list) else row.get("tags", ""),
                    "delivery_method": row.get("delivery_method", ""),
                    "reporter": row.get("reporter", ""),
                    "intelligence": row.get("intelligence", ""),
                    "download_url": row.get("download_url", ""),
                    "generated_at": generated_at,
                }
            )

    print(f"Exported {len(uniq_hashes)} unique hashes")
    print(f"- hashes: {hashes_out}")
    print(f"- csv:    {csv_out}")


if __name__ == "__main__":
    main()


